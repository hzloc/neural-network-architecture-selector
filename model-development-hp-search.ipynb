{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:00.391171Z","iopub.status.busy":"2024-07-23T17:28:00.389615Z","iopub.status.idle":"2024-07-23T17:28:12.338863Z","shell.execute_reply":"2024-07-23T17:28:12.337466Z","shell.execute_reply.started":"2024-07-23T17:28:00.391106Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'local_dkasc_development'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","\n","from scipy import stats\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.parametrizations import weight_norm\n","\n","import lightning.pytorch as pl\n","from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n","from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping, StochasticWeightAveraging\n","from lightning.pytorch.tuner import Tuner\n","\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","from typing import Callable, Optional, List, Tuple\n","\n","import pandas as pd\n","import numpy as np\n","\n","import pprint\n","from numpy.typing import ArrayLike\n","\n","from pathlib import Path\n","\n","import os\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_theme(style=\"ticks\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False})\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","wandb_api_key = os.environ.get(\"WANDB_API\")\n","os.environ.setdefault(\"WANDB_NOTEBOOK_NAME\", \"local_dkasc_development\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:12.341657Z","iopub.status.busy":"2024-07-23T17:28:12.340541Z","iopub.status.idle":"2024-07-23T17:28:14.824372Z","shell.execute_reply":"2024-07-23T17:28:14.822847Z","shell.execute_reply.started":"2024-07-23T17:28:12.341606Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find local_dkasc_development.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhz33co\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=wandb_api_key)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:14.827215Z","iopub.status.busy":"2024-07-23T17:28:14.826365Z","iopub.status.idle":"2024-07-23T17:28:14.834115Z","shell.execute_reply":"2024-07-23T17:28:14.832847Z","shell.execute_reply.started":"2024-07-23T17:28:14.827176Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.3.1+cpu\n","2.3.1\n"]}],"source":["print(torch.__version__)\n","print(pl.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Building source model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:14.837014Z","iopub.status.busy":"2024-07-23T17:28:14.836439Z","iopub.status.idle":"2024-07-23T17:28:14.851810Z","shell.execute_reply":"2024-07-23T17:28:14.850342Z","shell.execute_reply.started":"2024-07-23T17:28:14.836960Z"},"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","            features=['timestamp', 'Weather_Temperature_Celsius', 'Weather_Relative_Humidity', 'Global_Horizontal_Radiation', 'Active_Power'],\n","            label='Active_Power',\n","            input_size=12,\n","            horizon=1,\n","            batch_size=128,\n","            cv_iter=[0.5, 0.7, 0.9],\n","            train_dataset_size = 0.6,\n","            val_dataset_size=0.2,\n","            test_dataset_size=0.2,\n","            num_channels = [26] * 4,\n","            in_channels = 4,\n","            learning_rate=0.00002517,\n","            train=False\n","            )"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:14.854333Z","iopub.status.busy":"2024-07-23T17:28:14.853902Z","iopub.status.idle":"2024-07-23T17:28:14.871189Z","shell.execute_reply":"2024-07-23T17:28:14.869506Z","shell.execute_reply.started":"2024-07-23T17:28:14.854298Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set_theme(style=\"ticks\", rc={\"axes.spines.right\": False, \"axes.spines.top\": False})\n","\n","\n","def plot(historical, prediction, actual, ylabel=\"Radiation\"):\n","    x = np.arange(len(historical) + 1)\n","    print(prediction, actual)\n","    historical = np.append(historical, actual)\n","\n","    print(len(x), len(historical), print(x[-1]))\n","    plt.plot(x, historical, linestyle=\"--\", label=\"hist\")\n","    plt.scatter(x[-1], prediction, color=\"green\", marker=\"*\", label=\"pred\")\n","    plt.scatter(x[-1], actual, marker=\"o\", label=\"act\")\n","    axs = plt.gca()\n","    axs.yaxis.get_ticklocs(minor=True)\n","    axs.minorticks_on()\n","\n","    #     axs.tick_params(axis='x', which='minor', bottom=False)\n","    plt.legend()\n","    plt.title(\"Prediction of the radiation\")\n","    plt.ylabel(ylabel)\n","    plt.xlabel(\"Time steps\")\n","    plt.show()\n","\n","\n","def plot_pred_vs_act(preds, actuals, ylabel=\"Radiation\"):\n","    x = np.arange(len(preds))\n","    fig = plt.figure()\n","    plt.plot(x, preds, \"*g-\", label=\"pred\")\n","    plt.plot(x, actuals, \"xr-\", label=\"act\")\n","    plt.legend()\n","    plt.title(\"Prediction vs Actuals\")\n","    plt.ylabel(ylabel)\n","    plt.xlabel(\"Time steps\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:14.876219Z","iopub.status.busy":"2024-07-23T17:28:14.875656Z","iopub.status.idle":"2024-07-23T17:28:14.893008Z","shell.execute_reply":"2024-07-23T17:28:14.891281Z","shell.execute_reply.started":"2024-07-23T17:28:14.876178Z"},"trusted":true},"outputs":[],"source":["class DKACS(Dataset):\n","    def __init__(self, path: str, horizon: int, input_size: int, transform: Optional[List[Callable]]=None, target_transform: Optional[List[Callable]]=None,  data_path='./'):\n","        self.data: pd.DataFrame = pd.read_csv(path).values\n","#         self.data = data.values.astype(np.float32)\n","        self.h = horizon\n","        self.w = input_size\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.features, self.label = self.create_windows()\n","        \n","        \n","    def create_windows(self):\n","        total_possible_window_size = len(self.data) - self.w - self.h - 1\n","        features = np.zeros(shape=(total_possible_window_size, self.data.shape[1], self.w), dtype=np.float32)\n","        label = np.zeros(shape=(total_possible_window_size, self.h), dtype=np.float32)\n","        for i in range(total_possible_window_size):\n","            features[i] = np.transpose(self.data[i:i+self.w])\n","            label[i] = self.data[i+self.w+self.h-1, -1]\n","        return features, label\n","    \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, idx):\n","        features = torch.from_numpy(self.features[idx].astype(np.float32))\n","        label = torch.from_numpy(self.label[idx].astype(np.float32))\n","        \n","        return features, label"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:14.895308Z","iopub.status.busy":"2024-07-23T17:28:14.894825Z","iopub.status.idle":"2024-07-23T17:28:14.916476Z","shell.execute_reply":"2024-07-23T17:28:14.914949Z","shell.execute_reply.started":"2024-07-23T17:28:14.895267Z"},"trusted":true},"outputs":[],"source":["class DKASCDataModule(pl.LightningDataModule):\n","    def __init__(self, input_size: int = 72, horizon: int = 1, batch_size_dl: int = 32):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.batch_size_dl = batch_size_dl\n","        self.input_size = input_size\n","        self.horizon = horizon\n","    \n","    def prepare_data(self):\n","        pass\n","        \n","    def setup(self, stage=None):\n","        if stage == 'fit':\n","            self.dkasc_train = DKACS(\"./dataset/DKASC_train.csv\", self.horizon, self.input_size)\n","            self.dkasc_val = DKACS(\"./dataset/DKASC_val.csv\", self.horizon, self.input_size)\n","        \n","        if stage == 'test':\n","            self.dkasc_test = DKACS(\"./dataset/DKASC_test.csv\", self.horizon, self.input_size)\n","        \n","        if stage == 'predict':\n","            self.dkasc_test = DKACS(\"./dataset/DKASC_test.csv\", self.horizon, self.input_size)\n","\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.dkasc_train, shuffle=False, pin_memory=True, num_workers=3, persistent_workers=True, batch_size=self.batch_size_dl)\n","    \n","    def val_dataloader(self):\n","        return DataLoader(self.dkasc_val, shuffle=False, pin_memory=True, num_workers=3, persistent_workers=True,batch_size=self.batch_size_dl)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.dkasc_test, shuffle=False, pin_memory=True, num_workers=3, persistent_workers=True,batch_size=self.batch_size_dl)\n","    \n","    def predict_dataloader(self):\n","        return DataLoader(self.dkasc_test, shuffle=False, pin_memory=True, num_workers=3, persistent_workers=True,batch_size=self.batch_size_dl)\n","    \n","    \n","    \n","    def teardown(self, stage):\n","        if stage == 'fit':\n","            del self.dkasc_train\n","            del self.dkasc_val\n","            \n","        if stage == 'test':\n","            del self.dkasc_test"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-07-23T17:28:14.919663Z","iopub.status.busy":"2024-07-23T17:28:14.919268Z","iopub.status.idle":"2024-07-23T17:28:15.067065Z","shell.execute_reply":"2024-07-23T17:28:15.065304Z","shell.execute_reply.started":"2024-07-23T17:28:14.919630Z"},"trusted":true},"outputs":[],"source":["class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","class PermuteLayer(torch.nn.Module):\n","    dims: tuple[int, ...]\n","\n","    def __init__(self, dims: tuple[int, ...]) -> None:\n","        super().__init__()\n","        self.dims = dims\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","#         if not isinstance(input, torch.Tensor):\n","#             input = input[0]\n","        return input.permute(*self.dims)\n","    \n","class ECALayer(nn.Module):\n","    \"\"\"Constructs a ECA module.\n","\n","    Args:\n","        channel: Number of channels of the input feature map\n","        k_size: Adaptive selection of kernel size\n","    \"\"\"\n","    def __init__(self, channel, k_size=3):\n","        super(ECALayer, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.conv = nn.Conv1d(1, 1, kernel_size=k_size-1, padding=(k_size-1) // 2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # feature descriptor on the global spatial information\n","        y = self.avg_pool(x)\n","\n","        # Two different branches of ECA module\n","        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n","\n","        # Multi-scale information fusion\n","        y = self.sigmoid(y)\n","        \n","        return x * y.expand_as(x)\n","    \n","\n","activation_fn = dict(\n","    relu=nn.ReLU,\n","    tanh=nn.Tanh,\n","    leaky_relu=nn.LeakyReLU,\n","    sigmoid=nn.Sigmoid,\n","    elu=nn.ELU,\n","    gelu=nn.GELU,\n","    selu=nn.SELU,\n","    softmax=nn.Softmax,\n","    log_softmax=nn.LogSoftmax,\n",")\n","\n","kernel_init_fn = dict(\n","    xavier_uniform=nn.init.xavier_uniform_,\n","    xavier_normal=nn.init.xavier_normal_,\n","    kaiming_uniform=nn.init.kaiming_uniform_,\n","    kaiming_normal=nn.init.kaiming_normal_,\n","    normal=nn.init.normal_,\n","    uniform=nn.init.uniform_,\n",")\n","\n","def _check_activation_arg(\n","        activation,\n","        arg_name,\n","        ):\n","    if activation is None and arg_name == 'output_activation':\n","        return\n","    if isinstance( activation, str ):\n","        if activation not in activation_fn.keys():\n","            raise ValueError(\n","                f\"\"\"\n","                If argument '{arg_name}' is a string, it must be one of:\n","                {activation_fn.keys()}. However, you may also pass any\n","                torch.nn.Module object as the 'activation' argument.\n","                \"\"\"\n","                )\n","    elif not isinstance( activation, nn.Module ):\n","        raise ValueError(\n","            f\"\"\"\n","            The argument '{arg_name}' must either be a valid string or\n","            a torch.nn.Module object, but {activation} was passed,\n","            which is of type {type(activation)}.\n","            \"\"\"\n","            )\n","    return\n","\n","def _check_generic_input_arg(\n","        arg,\n","        arg_name,\n","        allowed_values,\n","        ):\n","    if arg not in allowed_values:\n","        raise ValueError(\n","            f\"\"\"\n","            Argument '{arg_name}' must be one of: {allowed_values},\n","            but {arg} was passed.\n","            \"\"\"\n","            )\n","    return\n","\n","def get_kernel_init_fn(\n","        name: str,\n","        activation: str,\n","        ) -> Tuple[ nn.Module, dict ]:\n","    if isinstance( activation, nn.Module ):\n","        return kernel_init_fn[ name ], dict()\n","    # TODO: this means no gain is used for custom activation functions\n","        \n","    if name not in kernel_init_fn.keys():\n","        raise ValueError(\n","            f\"Argument 'kernel_initializer' must be one of: {kernel_init_fn.keys()}\"\n","            )\n","    if name in [ 'xavier_uniform', 'xavier_normal' ]:\n","        if activation in [ 'gelu', 'elu', 'softmax', 'log_softmax' ]:\n","            warnings.warn(\n","                f\"\"\"\n","                Argument 'kernel_initializer' {name}\n","                is not compatible with activation {activation} in the\n","                sense that the gain is not calculated automatically.\n","                Here, a gain of sqrt(2) (like in ReLu) is used.\n","                This might lead to suboptimal results.\n","                \"\"\"\n","                )\n","            gain = np.sqrt( 2 )\n","        else:\n","            gain = nn.init.calculate_gain( activation )\n","        kernel_init_kw = dict( gain=gain )\n","    elif name in [ 'kaiming_uniform', 'kaiming_normal' ]:\n","        if activation in [ 'gelu', 'elu', 'softmax', 'log_softmax' ]:\n","            raise ValueError(\n","                f\"\"\"\n","                Argument 'kernel_initializer' {name}\n","                is not compatible with activation {activation}.\n","                It is recommended to use 'relu' or 'leaky_relu'.\n","                \"\"\"\n","                )\n","        else:\n","            nonlinearity = activation\n","        kernel_init_kw = dict( nonlinearity=nonlinearity )\n","    else:\n","        kernel_init_kw = dict()\n","    \n","    return kernel_init_fn[ name ], kernel_init_kw\n","\n","\n","\n","class CausalConv1d(nn.Conv1d):\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride = 1,\n","            dilation = 1,\n","            groups = 1,\n","            bias = True,\n","            buffer = None,\n","            lookahead = 0,\n","            **kwargs,\n","            ):\n","        \n","        super(CausalConv1d, self).__init__(\n","            in_channels = in_channels,\n","            out_channels = out_channels,\n","            kernel_size = kernel_size,\n","            stride = stride,\n","            padding = 0,\n","            dilation = dilation,\n","            groups = groups,\n","            bias = bias,\n","            **kwargs,\n","            )\n","        \n","        self.pad_len = (kernel_size - 1) * dilation\n","        if lookahead > self.pad_len//2:\n","            lookahead = self.pad_len//2\n","        self.lookahead = lookahead\n","\n","        self.buffer_len = self.pad_len - self.lookahead\n","        #print( 'pad len:', self.pad_len )\n","        #print( 'lookahead:', self.lookahead )\n","        #print( 'buffer len:', self.buffer_len )\n","        \n","        if buffer is None:\n","            buffer = torch.zeros(\n","                1,\n","                in_channels,\n","                self.pad_len,\n","                )\n","            \n","        self.register_buffer(\n","            'buffer',\n","            buffer,\n","            )\n","        \n","        return\n","    \n","    def _forward(self, x):\n","        p = nn.ConstantPad1d(\n","            ( self.buffer_len, self.lookahead ),\n","            0.0,\n","            )\n","        x = p(x)\n","        x = super().forward(x)\n","        return x\n","\n","    def forward(\n","            self,\n","            x,\n","            inference=False,\n","            ):\n","        if inference:\n","            x = self.inference(x)\n","        else:\n","            x = self._forward(x)\n","        return x\n","    \n","    def inference(self, x):\n","        if x.shape[0] != 1:\n","            raise ValueError(\n","                f\"\"\"\n","                Streaming inference of CausalConv1D layer only supports\n","                a batch size of 1, but batch size is {x.shape[0]}.\n","                \"\"\"\n","                )\n","        if x.shape[2] < self.lookahead + 1:\n","            raise ValueError(\n","                f\"\"\"\n","                Input time dimension {x.shape[2]} is too short for causal\n","                inference with lookahead {self.lookahead}. You must pass at\n","                least lookhead + 1 time steps ({self.lookahead + 1}).\n","                \"\"\"\n","                )\n","        x = torch.cat(\n","            (self.buffer, x),\n","            -1,\n","            )\n","        if self.lookahead > 0:\n","            self.buffer = x[:, :, -(self.pad_len+self.lookahead) : -self.lookahead ]\n","        else:\n","            self.buffer = x[:, :, -self.buffer_len: ]\n","        x = super().forward(x)\n","        return x\n","    \n","    def reset_buffer(self):\n","        self.buffer.zero_()\n","        if self.buffer.shape[2] != self.pad_len:\n","            raise ValueError(\n","                f\"\"\"\n","                Buffer shape {self.buffer.shape} does not match the expected\n","                shape (1, {self.in_channels}, {self.pad_len}).\n","                \"\"\"\n","                )\n","        return\n","\n","\n","\n","class TemporalConv1d(nn.Conv1d):\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=1,\n","            dilation=1,\n","            groups=1,\n","            bias=True,\n","            **kwargs,\n","            ):\n","        \n","        self.pad_len = (kernel_size-1) * dilation\n","\n","        super(TemporalConv1d, self).__init__(\n","            in_channels = in_channels,\n","            out_channels = out_channels,\n","            kernel_size = kernel_size,\n","            stride = stride,\n","            padding = self.pad_len // 2,\n","            dilation = dilation,\n","            groups = groups,\n","            bias = bias,\n","            **kwargs,\n","            )\n","        \n","        return\n","    \n","    def forward(self, x, inference=None):\n","        # Implementation of 'same'-type padding (non-causal padding)\n","    \n","        # Check if pad_len is an odd value\n","        # If so, pad the input one more on the right side\n","        if (self.pad_len % 2 != 0):\n","            x = F.pad(x, [0, 1])\n","\n","        x = super(TemporalConv1d, self).forward(x)\n","\n","        return x\n","\n","\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(\n","            self,\n","            n_inputs,\n","            n_outputs,\n","            kernel_size,\n","            stride,\n","            dilation,\n","            dropout,\n","            causal,\n","            use_norm,\n","            activation,\n","            kerner_initializer,\n","            embedding_shapes,\n","            embedding_mode,\n","            use_gate,\n","            lookahead,\n","            ):\n","        super(TemporalBlock, self).__init__()\n","        self.use_norm = use_norm\n","        self.activation = activation\n","        self.kernel_initializer = kerner_initializer\n","        self.embedding_shapes = embedding_shapes\n","        self.embedding_mode = embedding_mode\n","        self.use_gate = use_gate\n","        self.causal = causal\n","        self.lookahead = lookahead\n","\n","        if self.use_gate:\n","            conv1d_n_outputs = 2 * n_outputs\n","        else:\n","            conv1d_n_outputs = n_outputs\n","\n","        if self.causal:\n","            self.conv1 = CausalConv1d(\n","                in_channels=n_inputs,\n","                out_channels=conv1d_n_outputs,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                dilation=dilation,\n","                lookahead=self.lookahead,\n","                )\n","\n","            self.conv2 = CausalConv1d(\n","                in_channels=n_outputs,\n","                out_channels=n_outputs,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                dilation=dilation,\n","                lookahead=self.lookahead,\n","                )\n","\n","        else:\n","            self.conv1 = TemporalConv1d(\n","                in_channels=n_inputs,\n","                out_channels=conv1d_n_outputs,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                dilation=dilation,\n","                )\n","\n","            self.conv2 = TemporalConv1d(\n","                in_channels=n_outputs,\n","                out_channels=n_outputs,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                dilation=dilation,\n","                )\n","        \n","        if use_norm == 'batch_norm':\n","            if self.use_gate:\n","                self.norm1 = nn.BatchNorm1d(2 * n_outputs)\n","            else:\n","                self.norm1 = nn.BatchNorm1d(n_outputs)\n","            self.norm2 = nn.BatchNorm1d(n_outputs)\n","        elif use_norm == 'layer_norm':\n","            if self.use_gate:\n","                self.norm1 = nn.LayerNorm(2 * n_outputs)\n","            else:\n","                self.norm1 = nn.LayerNorm(n_outputs)\n","            self.norm2 = nn.LayerNorm(n_outputs)\n","        elif use_norm == 'weight_norm':\n","            self.norm1 = None\n","            self.norm2 = None\n","            self.conv1 = weight_norm(self.conv1)\n","            self.conv2 = weight_norm(self.conv2)\n","        elif use_norm is None:\n","            self.norm1 = None\n","            self.norm2 = None\n","\n","        if isinstance( self.activation, str ):\n","            self.activation1 = activation_fn[ self.activation ]()\n","            self.activation2 = activation_fn[ self.activation ]()\n","            self.activation_final = activation_fn[ self.activation ]()\n","        else:\n","            self.activation1 = self.activation()\n","            self.activation2 = self.activation()\n","            self.activation_final = self.activation()\n","\n","        if self.use_gate:\n","            self.activation1 = nn.GLU(dim=1)\n","\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        \n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1, padding=0) if n_inputs != n_outputs else None\n","\n","        if self.embedding_shapes is not None:\n","            if self.use_gate:\n","                embedding_layer_n_outputs = 2 * n_outputs\n","            else:\n","                embedding_layer_n_outputs = n_outputs\n","\n","            self.embedding_projection_1 = nn.Conv1d(\n","                in_channels = sum( [ shape[0] for shape in self.embedding_shapes ] ),\n","                out_channels = embedding_layer_n_outputs,\n","                kernel_size = 1,\n","                )\n","            \n","            self.embedding_projection_2 = nn.Conv1d(\n","                in_channels = 2 * embedding_layer_n_outputs,\n","                out_channels = embedding_layer_n_outputs,\n","                kernel_size = 1,\n","                )\n","        \n","        self.init_weights()\n","        return\n","\n","    def init_weights(self):\n","        initialize, kwargs = get_kernel_init_fn(\n","            name=self.kernel_initializer,\n","            activation=self.activation,\n","            )\n","        initialize(\n","            self.conv1.weight,\n","            **kwargs\n","            )\n","        initialize(\n","            self.conv2.weight,\n","            **kwargs\n","            )\n","\n","        if self.downsample is not None:\n","            initialize(\n","                self.downsample.weight,\n","                **kwargs\n","                )\n","        return\n","    \n","    def apply_norm(\n","            self,\n","            norm_fn,\n","            x,\n","        ):\n","        if self.use_norm == 'batch_norm':\n","            x = norm_fn(x)\n","        elif self.use_norm == 'layer_norm':\n","            x = norm_fn( x.transpose(1, 2) )\n","            x = x.transpose(1, 2)\n","        return x\n","    \n","    def apply_embeddings(\n","            self,\n","            x,\n","            embeddings,\n","            ):\n","        \n","        if not isinstance( embeddings, list ):\n","            embeddings = [ embeddings ]\n","\n","        e = []\n","        for embedding, expected_shape in zip( embeddings, self.embedding_shapes ):\n","            if embedding.shape[1] != expected_shape[0]:\n","                raise ValueError(\n","                    f\"\"\"\n","                    Embedding shape {embedding.shape} passed to 'forward' does not \n","                    match the expected shape {expected_shape} provided as input to\n","                    argument 'embedding_shapes'.\n","                    \"\"\"\n","                    )\n","            if len( embedding.shape ) == 2:\n","                # unsqueeze time dimension of e and repeat it to match x\n","                e.append( embedding.unsqueeze(2).repeat(1, 1, x.shape[2]) )\n","            elif len( embedding.shape ) == 3:\n","                # check if time dimension of embedding matches x\n","                if embedding.shape[2] != x.shape[2]:\n","                    raise ValueError(\n","                        f\"\"\"\n","                        Embedding time dimension {embedding.shape[2]} does not\n","                        match the input time dimension {x.shape[2]}\n","                        \"\"\"\n","                        )\n","                e.append( embedding )\n","        e = torch.cat( e, dim=1 )\n","        e = self.embedding_projection_1( e )\n","        #print('shapes:', e.shape, x.shape)\n","        if self.embedding_mode == 'concat':\n","            x = self.embedding_projection_2(\n","                torch.cat( [ x, e ], dim=1 )\n","                )\n","        elif self.embedding_mode == 'add':\n","            x = x + e\n","\n","        return x\n","    \n","    def forward(\n","            self,\n","            x,\n","            embeddings,\n","            inference,\n","            ):\n","        out = self.conv1(x, inference=inference)\n","        out = self.apply_norm( self.norm1, out )\n","\n","        if embeddings is not None:\n","            out = self.apply_embeddings( out, embeddings )\n","\n","        out = self.activation1(out)\n","        out = self.dropout1(out)\n","\n","        out = self.conv2(out, inference=inference)\n","        out = self.apply_norm( self.norm2, out )\n","        out = self.activation2(out)\n","        out = self.dropout2(out)\n","\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.activation_final(out + res), out\n","    \n","    def inference(\n","            self,\n","            x,\n","            embeddings,\n","            ):\n","        if not self.causal:\n","            raise ValueError(\n","                \"\"\"\n","                This streaming inference mode is made for blockwise causal\n","                processing and thus, is only supported for causal networks.\n","                However, you selected a non-causal network.\n","                \"\"\"\n","                )\n","        x, out = self.forward(x, embeddings, inference=True)\n","        return x, out\n","\n","\n","\n","class TCN(nn.Module):\n","    def __init__(\n","            self,\n","            num_inputs: int,\n","            num_channels: ArrayLike,\n","            kernel_size: int = 4,\n","            dilations: Optional[ ArrayLike ] = None,\n","            dilation_reset: Optional[ int ] = None,\n","            dropout: float = 0.1,\n","            causal: bool = True,\n","            use_norm: str = 'weight_norm',\n","            activation: str = 'relu',\n","            kernel_initializer: str = 'xavier_uniform',\n","            use_skip_connections: bool = False,\n","            input_shape: str = 'NCL',\n","            embedding_shapes: Optional[ ArrayLike ] = None,\n","            embedding_mode: str = 'add',\n","            use_gate: bool = False,\n","            lookahead: int = 0,\n","            output_projection: Optional[ int ] = None,\n","            output_activation: Optional[ str ] = None,\n","            ):\n","        super(TCN, self).__init__()\n","\n","        if dilations is not None and len(dilations) != len(num_channels):\n","            raise ValueError(\"Length of dilations must match length of num_channels\")\n","        \n","        self.allowed_norm_values = ['batch_norm', 'layer_norm', 'weight_norm', None]\n","        self.allowed_input_shapes = ['NCL', 'NLC']\n","\n","        _check_generic_input_arg( causal, 'causal', [True, False] )\n","        _check_generic_input_arg( use_norm, 'use_norm', self.allowed_norm_values )\n","        _check_activation_arg(activation, 'activation')\n","        _check_generic_input_arg( kernel_initializer, 'kernel_initializer', kernel_init_fn.keys() )\n","        _check_generic_input_arg( use_skip_connections, 'use_skip_connections', [True, False] )\n","        _check_generic_input_arg( input_shape, 'input_shape', self.allowed_input_shapes )\n","        _check_generic_input_arg( embedding_mode, 'embedding_mode', ['add', 'concat'] )\n","        _check_generic_input_arg( use_gate, 'use_gate', [True, False] )\n","        _check_activation_arg(output_activation, 'output_activation')\n","\n","        if dilations is None:\n","            if dilation_reset is None:\n","                dilations = [ 2 ** i for i in range( len( num_channels ) ) ]\n","            else:\n","                # Calculate after which layers to reset\n","                dilation_reset = int( np.log2( dilation_reset * 2 ) )\n","                dilations = [\n","                    2 ** (i % dilation_reset)\n","                    for i in range( len( num_channels ) )\n","                    ]\n","            \n","        self.dilations = dilations\n","        self.activation = activation\n","        self.kernel_initializer = kernel_initializer\n","        self.use_skip_connections = use_skip_connections\n","        self.input_shape = input_shape\n","        self.embedding_shapes = embedding_shapes\n","        self.embedding_mode = embedding_mode\n","        self.use_gate = use_gate\n","        self.causal = causal\n","        self.lookahead = lookahead\n","        self.output_projection = output_projection\n","        self.output_activation = output_activation\n","\n","        if embedding_shapes is not None:\n","            if isinstance(embedding_shapes, Iterable):\n","                for shape in embedding_shapes:\n","                    if not isinstance( shape, tuple ):\n","                        try:\n","                            shape = tuple( shape )\n","                        except Exception as e:\n","                            raise ValueError(\n","                                f\"\"\"\n","                                Each shape in argument 'embedding_shapes' must be an Iterable of tuples.\n","                                Tried to convert {shape} to tuple, but failed with error: {e}\n","                                \"\"\"\n","                                )\n","                    if len( shape ) not in [ 1, 2 ]:\n","                        raise ValueError(\n","                            f\"\"\"\n","                            Tuples in argument 'embedding_shapes' must be of length 1 or 2.\n","                            One-dimensional tuples are interpreted as (embedding_dim,) and\n","                            two-dimensional tuples as (embedding_dim, time_steps).\n","                            \"\"\"\n","                            )\n","            else:\n","                raise ValueError(\n","                    f\"\"\"\n","                    Argument 'embedding_shapes' must be an Iterable of tuples,\n","                    but is {type(embedding_shapes)}.\n","                    \"\"\"\n","                    )\n","            \n","\n","        if use_skip_connections:\n","            self.downsample_skip_connection = nn.ModuleList()\n","            for i in range( len( num_channels ) ):\n","                # Downsample layer output dim to network output dim if needed\n","                if num_channels[i] != num_channels[-1]:\n","                    self.downsample_skip_connection.append(\n","                        nn.Conv1d( num_channels[i], num_channels[-1], 1 )\n","                        )\n","                else:\n","                    self.downsample_skip_connection.append( None )\n","            self.init_skip_connection_weights()\n","            if isinstance( self.activation, str ):\n","                self.activation_skip_out = activation_fn[ self.activation ]()\n","            else:\n","                self.activation_skip_out = self.activation()\n","        else:\n","            self.downsample_skip_connection = None\n","        \n","        layers = []\n","        num_levels = len(num_channels)\n","        \n","        for i in range(num_levels):\n","            dilation_size = self.dilations[i]\n","\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","\n","            layers += [\n","                TemporalBlock(\n","                    n_inputs=in_channels,\n","                    n_outputs=out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=1,\n","                    dilation=dilation_size,\n","                    dropout=dropout,\n","                    causal=causal,\n","                    use_norm=use_norm,\n","                    activation=activation,\n","                    kerner_initializer=self.kernel_initializer,\n","                    embedding_shapes=self.embedding_shapes,\n","                    embedding_mode=self.embedding_mode,\n","                    use_gate=self.use_gate,\n","                    lookahead=self.lookahead,\n","                    )\n","                ]\n","\n","        self.network = nn.ModuleList(layers)\n","\n","        if self.output_projection is not None:\n","            self.projection_out = nn.Conv1d(\n","                in_channels=num_channels[-1],\n","                out_channels=self.output_projection,\n","                kernel_size=1,\n","                )\n","        else:\n","            self.projection_out = None\n","\n","        if self.output_activation is not None:\n","            if isinstance( self.output_activation, str ):\n","                self.activation_out = activation_fn[ self.output_activation ]()\n","            else:\n","                self.activation_out = self.output_activation()\n","        else:\n","            self.activation_out = None #nn.Identity()\n","\n","        if self.causal:\n","            self.reset_buffers()\n","        return\n","    \n","    def init_skip_connection_weights(self):\n","        initialize, kwargs = get_kernel_init_fn(\n","            name=self.kernel_initializer,\n","            activation=self.activation,\n","            )\n","        for layer in self.downsample_skip_connection:\n","            if layer is not None:\n","                initialize(\n","                    layer.weight,\n","                    **kwargs\n","                    )\n","        return\n","\n","    def forward(\n","            self,\n","            x,\n","            embeddings=None,\n","            inference=False,\n","            ):\n","        if inference and not self.causal:\n","            raise ValueError(\n","                \"\"\"\n","                This streaming inference mode is made for blockwise causal\n","                processing and thus, is only supported for causal networks.\n","                However, you selected a non-causal network.\n","                \"\"\"\n","                )\n","        if self.input_shape == 'NLC':\n","            x = x.transpose(1, 2)\n","        if self.use_skip_connections:\n","            skip_connections = []\n","            # Adding skip connections from each layer to the output\n","            # Excluding the last layer, as it would not skip trainable weights\n","            for index, layer in enumerate( self.network ):\n","                x, skip_out = layer(\n","                    x,\n","                    embeddings=embeddings,\n","                    inference=inference,\n","                    )\n","                if self.downsample_skip_connection[ index ] is not None:\n","                    skip_out = self.downsample_skip_connection[ index ]( skip_out )\n","                if index < len( self.network ) - 1:\n","                    skip_connections.append( skip_out )\n","            skip_connections.append( x )\n","            x = torch.stack( skip_connections, dim=0 ).sum( dim=0 )\n","            x = self.activation_skip_out( x )\n","        else:\n","            for layer in self.network:\n","                #print( 'TCN, embeddings:', embeddings.shape )\n","                x, _ = layer(\n","                    x,\n","                    embeddings=embeddings,\n","                    inference=inference,\n","                    )\n","        if self.projection_out is not None:\n","            x = self.projection_out( x )\n","        if self.activation_out is not None:\n","            x = self.activation_out( x )\n","        if inference and self.lookahead > 0:\n","            x = x[ :, :, self.lookahead: ]\n","        if self.input_shape == 'NLC':\n","            x = x.transpose(1, 2)\n","        return x\n","    \n","    def inference(\n","            self,\n","            x,\n","            embeddings=None,\n","            ):\n","        x = self.forward(\n","            x,\n","            embeddings=embeddings,\n","            inference=True,\n","            )\n","        return x\n","    \n","    def reset_buffers(self):\n","        def _reset_buffer(x):\n","            if isinstance(x, CausalConv1d):\n","                x.reset_buffer()\n","        self.apply(_reset_buffer)\n","        return\n","\n","class TCNETANetLSTM(pl.LightningModule):\n","    def __init__(self, in_channels, num_channels: list[int], kernel_size: int=2, dropout: int=0.1, learning_rate=0.001, batch_size=32, hp_search=False):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.lr = learning_rate\n","        self.tcn = TCN(in_channels, num_channels)\n","        self.eca = ECALayer(channel=num_channels[0], k_size=kernel_size)\n","        self.network = nn.Sequential(self.tcn, self.eca)\n","        self.gru = nn.LSTM(num_channels[0], 16, 2, batch_first=True)\n","        self.permute_layer = PermuteLayer(dims=(0,-1,1))\n","        self.flat = nn.Flatten()\n","        self.linear = nn.Linear(1152, 1)\n","        if hp_search is True:\n","            self.linear = nn.LazyLinear(1)\n","            \n","        \n","    def forward(self, inp: torch.Tensor):\n","        y = self.network(inp[0])\n","        y = self.permute_layer(y)\n","        y = self.gru(y)[0]\n","        if len(y) > 0:\n","            y = self.flat(y)\n","        y = self.linear(y)\n","        return y\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        return optimizer\n","\n","    def training_step(self, batch, batch_idx):\n","      # training_step defines the train loop.\n","        x, y = batch\n","        x_hat = self.network(x)\n","        x_hat = self.permute_layer(x_hat)\n","        x_hat = self.gru(x_hat)\n","        x_hat = self.flat(x_hat[0])\n","        x_hat = self.linear(x_hat)\n","        loss = nn.functional.l1_loss(x_hat, y)\n","        self.log('loss', loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        x_hat = self.network(x)\n","        x_hat = self.permute_layer(x_hat)\n","        x_hat = self.gru(x_hat)\n","        x_hat = self.flat(x_hat[0])\n","        x_hat = self.linear(x_hat)\n","        loss = nn.functional.l1_loss(x_hat, y)\n","        loss_mse = nn.functional.mse_loss(x_hat, y)\n","        self.log('test_MAE_loss', loss, logger=True)\n","        self.log(\"test_mse_loss\", loss_mse, logger=True)\n","        return {\"prediction\": x_hat, \"actual\": y, \"loss_mae\": loss, \"loss_mse\": loss_mse}\n","\n","    def validation_step(self, batch, batch_idx):\n","      # training_step defines the train loop.\n","        x, y = batch\n","        x_hat = self.network(x)\n","        x_hat = self.permute_layer(x_hat)\n","        x_hat = self.gru(x_hat)\n","        x_hat = self.flat(x_hat[0])\n","        x_hat = self.linear(x_hat)\n","        loss = nn.functional.l1_loss(x_hat, y)\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","    \n","    def predict_step(self, *args, **kwargs):\n","        batch = kwargs.get(\"batch\", args[0])\n","        predictions = self(batch)\n","        actual_values = batch[1]\n","        return predictions, actual_values\n","    \n","    \n","    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n","        plot_pred_vs_act(outputs['prediction'], outputs['actual'], ylabel=\"Power ratio output\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.069123Z","iopub.status.busy":"2024-07-23T17:28:15.068653Z","iopub.status.idle":"2024-07-23T17:28:15.075934Z","shell.execute_reply":"2024-07-23T17:28:15.074284Z","shell.execute_reply.started":"2024-07-23T17:28:15.069087Z"},"trusted":true},"outputs":[],"source":["dkasc_datamodule = DKASCDataModule(batch_size_dl=CONFIG[\"batch_size\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.077880Z","iopub.status.busy":"2024-07-23T17:28:15.077436Z","iopub.status.idle":"2024-07-23T17:28:15.171561Z","shell.execute_reply":"2024-07-23T17:28:15.170291Z","shell.execute_reply.started":"2024-07-23T17:28:15.077838Z"},"trusted":true},"outputs":[],"source":["tcnecanet = TCNETANetLSTM(in_channels=CONFIG[\"in_channels\"], num_channels=CONFIG[\"num_channels\"])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.173546Z","iopub.status.busy":"2024-07-23T17:28:15.173154Z","iopub.status.idle":"2024-07-23T17:28:15.187095Z","shell.execute_reply":"2024-07-23T17:28:15.185438Z","shell.execute_reply.started":"2024-07-23T17:28:15.173514Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["callbacks = [\n","    ModelCheckpoint(monitor=\"val_loss\", filename='{epoch}-{val_loss:.2f}'),\n","    EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n","]"]},{"cell_type":"markdown","metadata":{},"source":["## Cyclical Learning rate"]},{"cell_type":"markdown","metadata":{},"source":["### Find initial learning rate range for parameter search"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.189162Z","iopub.status.busy":"2024-07-23T17:28:15.188682Z","iopub.status.idle":"2024-07-23T17:28:15.195129Z","shell.execute_reply":"2024-07-23T17:28:15.193807Z","shell.execute_reply.started":"2024-07-23T17:28:15.189124Z"},"trusted":true},"outputs":[],"source":["# wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\",\n","#                                log_model=\"all\",\n","#                                prefix=\"source_model\",\n","#                                config=CONFIG,\n","#                                reinit=True,\n","#                                notes=\"Cyclical Learning Rate\"\n","#                               )"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.197846Z","iopub.status.busy":"2024-07-23T17:28:15.197359Z","iopub.status.idle":"2024-07-23T17:28:15.210171Z","shell.execute_reply":"2024-07-23T17:28:15.208362Z","shell.execute_reply.started":"2024-07-23T17:28:15.197812Z"},"trusted":true},"outputs":[],"source":["# dkasc_datamodule = DKASCDataModule(CONFIG['input_size'], CONFIG['horizon'], batch_size=CONFIG[\"batch_size\"])\n","# model = TCNETANetLSTM(in_channels=CONFIG['in_channels'], num_channels=CONFIG['num_channels'], learning_rate=CONFIG['learning_rate'])\n","# trainer = pl.Trainer(limit_train_batches=0.5, limit_val_batches=0.5, logger=wandb_logger, callbacks=callbacks, enable_progress_bar=True)\n","# tuner = Tuner(trainer)\n","# lr_finder = tuner.lr_find(model, datamodule=dkasc_datamodule, num_training=500, max_lr=3, early_stop_threshold=None)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.212474Z","iopub.status.busy":"2024-07-23T17:28:15.212052Z","iopub.status.idle":"2024-07-23T17:28:15.230995Z","shell.execute_reply":"2024-07-23T17:28:15.229362Z","shell.execute_reply.started":"2024-07-23T17:28:15.212442Z"},"trusted":true},"outputs":[],"source":["# lr_finder.plot(suggest=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.233167Z","iopub.status.busy":"2024-07-23T17:28:15.232682Z","iopub.status.idle":"2024-07-23T17:28:15.244232Z","shell.execute_reply":"2024-07-23T17:28:15.242940Z","shell.execute_reply.started":"2024-07-23T17:28:15.233120Z"},"trusted":true},"outputs":[],"source":["# wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter search"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.246724Z","iopub.status.busy":"2024-07-23T17:28:15.246184Z","iopub.status.idle":"2024-07-23T17:28:15.257744Z","shell.execute_reply":"2024-07-23T17:28:15.256336Z","shell.execute_reply.started":"2024-07-23T17:28:15.246677Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import ParameterSampler\n","from scipy.stats import loguniform\n","from datetime import datetime\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.260022Z","iopub.status.busy":"2024-07-23T17:28:15.259509Z","iopub.status.idle":"2024-07-23T17:28:15.274980Z","shell.execute_reply":"2024-07-23T17:28:15.273482Z","shell.execute_reply.started":"2024-07-23T17:28:15.259979Z"},"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","            features=[['timestamp', 'Weather_Temperature_Celsius', 'Weather_Relative_Humidity', 'Global_Horizontal_Radiation', 'Active_Power']],\n","            label=['Active_Power'],\n","            input_size=[12, 24, 32, 48, 60, 72, 84, 96],\n","            horizon=[1],\n","            batch_size=[32, 64, 128],\n","            num_channels = [[x] * 4 for x in range(20, 36, 2)],\n","            in_channels = [4],\n","            learning_rate=loguniform(1e-5, 5e-5),\n","            train=[True]\n","            )\n","\n","sampler = ParameterSampler(CONFIG, n_iter=40, random_state=132)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.277437Z","iopub.status.busy":"2024-07-23T17:28:15.276614Z","iopub.status.idle":"2024-07-23T17:28:15.288873Z","shell.execute_reply":"2024-07-23T17:28:15.287288Z","shell.execute_reply.started":"2024-07-23T17:28:15.277401Z"},"trusted":true},"outputs":[],"source":["sweep_config = {\n","    \"method\": \"bayes\",\n","    \"metric\": {\"goal\": \"minimize\", \"name\": \"source_model-val_loss\"},\n","    \"parameters\": {\n","        'batch_size': {\n","            'distribution': 'q_log_uniform_values',\n","            'q': 8,\n","            'min': 32,\n","            'max': 256},\n","        \"learning_rate\": {\"distribution\": \"log_uniform\", \"max\": -4.46, \"min\": -11.6},\n","        \"input_size\": {\n","            'distribution': 'q_log_uniform_values',\n","            'q': 2,\n","            'min': 12,\n","            'max': 96},\n","        \"horizon\": {\"value\": 1},\n","        \"in_channels\": {\"value\": 4},\n","        \"num_channels\": {\n","            'distribution': 'q_log_uniform_values',\n","            'q': 4,\n","            'min': 12,\n","            'max': 80\n","        }\n","    },\n","}"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.291437Z","iopub.status.busy":"2024-07-23T17:28:15.290844Z","iopub.status.idle":"2024-07-23T17:28:15.304585Z","shell.execute_reply":"2024-07-23T17:28:15.303273Z","shell.execute_reply.started":"2024-07-23T17:28:15.291390Z"},"trusted":true},"outputs":[],"source":["def sweep_it(config=None):\n","    wandb.init()\n","    wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\",\n","                                   log_model=\"all\",\n","                                   prefix=\"source_model\",\n","                                   config=config,\n","                                   reinit=True\n","                                  )\n","    config = wandb.config\n","    wandb_logger.log_metrics\n","    try:\n","        dkasc_datamodule = DKASCDataModule(config['input_size'], config['horizon'], batch_size=config[\"batch_size\"])\n","        model = TCNETANetLSTM(in_channels=config['in_channels'], num_channels=[config['num_channels']] * 4, learning_rate=config['learning_rate'], batch_size =config['batch_size'], hp_search=True)\n","        trainer = pl.Trainer(max_epochs=50, limit_train_batches=0.5, limit_val_batches=0.5, logger=wandb_logger, enable_progress_bar=True)\n","        start = datetime.now()\n","        trainer.fit(model, dkasc_datamodule)\n","        end = datetime.now()\n","        print(f\"Training took {end - start} seconds\")\n","        trainer.test(model, dkasc_datamodule)\n","    except Exception as exc:\n","        print(exc)       \n","        exit()\n","    finally:\n","        wandb.finish()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.311521Z","iopub.status.busy":"2024-07-23T17:28:15.311036Z","iopub.status.idle":"2024-07-23T17:28:15.322129Z","shell.execute_reply":"2024-07-23T17:28:15.320829Z","shell.execute_reply.started":"2024-07-23T17:28:15.311485Z"},"trusted":true},"outputs":[],"source":["sweep = False\n","if sweep is True:\n","    sweep_id = wandb.sweep(sweep_config, project=\"Transfer Learning DKASC dataset training\",)    \n","    wandb.agent(sweep_id, sweep_it, count=75)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Search"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.324160Z","iopub.status.busy":"2024-07-23T17:28:15.323653Z","iopub.status.idle":"2024-07-23T17:28:15.337322Z","shell.execute_reply":"2024-07-23T17:28:15.336037Z","shell.execute_reply.started":"2024-07-23T17:28:15.324119Z"},"trusted":true},"outputs":[],"source":["hp_search = False\n","if hp_search is True:\n","    for ind, s in enumerate(sampler):\n","        if ind < 9:\n","            continue\n","        print(ind, s)\n","        wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\",\n","                                   log_model=\"all\",\n","                                   prefix=\"source_model\",\n","                                   config=s,\n","                                   reinit=True\n","                                  )\n","        try:\n","            dkasc_datamodule = DKASCDataModule(s['input_size'], s['horizon'], batch_size=s[\"batch_size\"])\n","            model = TCNETANetLSTM(in_channels=s['in_channels'], num_channels=s['num_channels'], learning_rate=s['learning_rate'], batch_size =s['batch_size'], hp_search=True)\n","            callbacks = [\n","                ModelCheckpoint(monitor=\"val_loss\", filename='{epoch}-{val_loss:.2f}'),\n","            ]\n","            trainer = pl.Trainer(max_epochs=1, limit_train_batches=0.5, limit_val_batches=0.5, logger=wandb_logger, callbacks=callbacks, enable_progress_bar=True)\n","            start = datetime.now()\n","            trainer.fit(model, dkasc_datamodule)\n","            end = datetime.now()\n","            print(f\"Training took {end - start} seconds\")\n","            trainer.test(model, dkasc_datamodule)\n","        except Exception as exc:\n","            print(exc)       \n","            exit()\n","        finally:\n","            wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.339864Z","iopub.status.busy":"2024-07-23T17:28:15.339420Z","iopub.status.idle":"2024-07-23T17:28:15.356631Z","shell.execute_reply":"2024-07-23T17:28:15.355383Z","shell.execute_reply.started":"2024-07-23T17:28:15.339829Z"},"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","            features=['timestamp', 'Weather_Temperature_Celsius', 'Weather_Relative_Humidity', 'Global_Horizontal_Radiation', 'Active_Power'],\n","            label='Active_Power',\n","            input_size=36,\n","            horizon=1,\n","            batch_size=152,\n","            cv_iter=[0.5, 0.7, 0.9],\n","            train_dataset_size = 0.6,\n","            val_dataset_size=0.2,\n","            test_dataset_size=0.2,\n","            num_channels = [20] * 4,\n","            in_channels = 4,\n","            learning_rate=0.004598,\n","            train=False,\n","            test=False\n","            )"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.358699Z","iopub.status.busy":"2024-07-23T17:28:15.358293Z","iopub.status.idle":"2024-07-23T17:28:15.370991Z","shell.execute_reply":"2024-07-23T17:28:15.369651Z","shell.execute_reply.started":"2024-07-23T17:28:15.358668Z"},"trusted":true},"outputs":[],"source":["callbacks = [\n","    ModelCheckpoint(monitor=\"val_loss\", filename='{epoch}-{val_loss:.2f}'),\n","    EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n","]"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.373854Z","iopub.status.busy":"2024-07-23T17:28:15.372704Z","iopub.status.idle":"2024-07-23T17:28:15.944527Z","shell.execute_reply":"2024-07-23T17:28:15.942936Z","shell.execute_reply.started":"2024-07-23T17:28:15.373794Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["Training took 0:00:00 seconds\n"]}],"source":["try:\n","    wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\",\n","                                   log_model=\"all\",\n","                                   prefix=\"source_model\",\n","                                   config=CONFIG,\n","                                   reinit=True,\n","                                   notes=\"Training the model with best hp parameters\"\n","                                  )\n","    dkasc_datamodule = DKASCDataModule(CONFIG['input_size'], CONFIG['horizon'], batch_size_dl=CONFIG[\"batch_size\"])\n","    model = TCNETANetLSTM(in_channels=CONFIG['in_channels'], num_channels=CONFIG['num_channels'], learning_rate=CONFIG['learning_rate'], batch_size=CONFIG['batch_size'], hp_search=True)\n","    callbacks = [\n","                ModelCheckpoint(monitor=\"val_loss\", filename='{epoch}-{val_loss:.2f}'),\n","            ]\n","    trainer = pl.Trainer(max_epochs=1000, logger=wandb_logger, callbacks=callbacks, enable_progress_bar=True)\n","    start = datetime.now()\n","    if CONFIG[\"train\"] == True:\n","        trainer.fit(model, dkasc_datamodule)\n","    end = datetime.now()\n","    print(f\"Training took {end - start} seconds\")\n","\n","    if CONFIG[\"test\"] == True:\n","        model_id = \"model-dzwl3pzf\"\n","        local_artifacts = list(Path(\"\").glob(f\"**/{model_id}*\"))\n","        if not local_artifacts:\n","            artifact = wandb.run.use_artifact(f'hz33co/Transfer Learning DKASC dataset training/{model_id}:best', type='model')\n","            artifact_dir = artifact.download()\n","        else:\n","            artifact_dir = local_artifacts[-1]\n","        model = TCNETANetLSTM.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")\n","        trainer.test(model, dkasc_datamodule)\n","except Exception as exc:\n","    print(exc)       \n","    exit()\n","finally:\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.947716Z","iopub.status.busy":"2024-07-23T17:28:15.946627Z","iopub.status.idle":"2024-07-23T17:28:15.955111Z","shell.execute_reply":"2024-07-23T17:28:15.952837Z","shell.execute_reply.started":"2024-07-23T17:28:15.947663Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.17.5 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>c:\\Users\\husey\\Documents\\learn\\master\\wandb\\run-20240723_212657-u0vgnjcr</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training/runs/u0vgnjcr' target=\"_blank\">eager-donkey-170</a></strong> to <a href='https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training' target=\"_blank\">https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training/runs/u0vgnjcr' target=\"_blank\">https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training/runs/u0vgnjcr</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hz33co/Transfer%20Learning%20DKASC%20dataset%20training/runs/u0vgnjcr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x1c8a36ed630>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"Transfer Learning DKASC dataset training\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:15.958429Z","iopub.status.busy":"2024-07-23T17:28:15.957538Z","iopub.status.idle":"2024-07-23T17:28:16.128574Z","shell.execute_reply":"2024-07-23T17:28:16.127327Z","shell.execute_reply.started":"2024-07-23T17:28:15.958364Z"},"trusted":true},"outputs":[],"source":["model_id = \"model-dzwl3pzf\"\n","# model_id = \"model-0xv5a270\"\n","local_artifacts = list(Path(\"\").glob(f\"**/{model_id}*\"))\n","if not local_artifacts:\n","    artifact = wandb.run.use_artifact(f'hz33co/Transfer Learning DKASC dataset training/{model_id}:best', type='model')\n","    artifact_dir = artifact.download()\n","else:\n","    artifact_dir = local_artifacts[-1]\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:16.130316Z","iopub.status.busy":"2024-07-23T17:28:16.129956Z","iopub.status.idle":"2024-07-23T17:28:16.138356Z","shell.execute_reply":"2024-07-23T17:28:16.136943Z","shell.execute_reply.started":"2024-07-23T17:28:16.130286Z"},"trusted":true},"outputs":[{"data":{"text/plain":["WindowsPath('artifacts/model-dzwl3pzf-v17')"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["artifact_dir"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:16.140993Z","iopub.status.busy":"2024-07-23T17:28:16.140423Z","iopub.status.idle":"2024-07-23T17:28:16.152516Z","shell.execute_reply":"2024-07-23T17:28:16.151126Z","shell.execute_reply.started":"2024-07-23T17:28:16.140950Z"},"trusted":true},"outputs":[],"source":["wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\",\n","                                   log_model=\"all\",\n","                                   prefix=\"source_model\",\n","                                   config=CONFIG,\n","                                   reinit=True,\n","                                   notes=\"Training the model with best hp parameters\"\n","                                  )"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:16.155018Z","iopub.status.busy":"2024-07-23T17:28:16.154525Z","iopub.status.idle":"2024-07-23T17:28:16.595912Z","shell.execute_reply":"2024-07-23T17:28:16.594650Z","shell.execute_reply.started":"2024-07-23T17:28:16.154976Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\utilities\\migration\\utils.py:56: The loaded checkpoint was produced with Lightning v2.3.3, which is newer than your current Lightning version: v2.3.1\n","c:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n","Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n"]}],"source":["model = TCNETANetLSTM.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")\n","trainer = pl.Trainer(max_epochs=1000, logger=wandb_logger, callbacks=callbacks, enable_progress_bar=True)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:16.597817Z","iopub.status.busy":"2024-07-23T17:28:16.597410Z","iopub.status.idle":"2024-07-23T17:28:43.055434Z","shell.execute_reply":"2024-07-23T17:28:43.053559Z","shell.execute_reply.started":"2024-07-23T17:28:16.597783Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"]}],"source":["trainer.test(model, datamodule=dkasc_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Error while merging hparams: the keys ['batch_size'] are present in both the LightningModule's and LightningDataModule's hparams but have different values.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdkasc_datamodule\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:863\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[1;32mc:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:902\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[1;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    899\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m )\n\u001b[1;32m--> 902\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:969\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    966\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    967\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 969\u001b[0m \u001b[43m_log_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrestore_checkpoint_after_setup:\n\u001b[0;32m    972\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\husey\\miniconda3\\envs\\master\\lib\\site-packages\\lightning\\pytorch\\loggers\\utilities.py:80\u001b[0m, in \u001b[0;36m_log_hyperparams\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     78\u001b[0m             inconsistent_keys\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inconsistent_keys:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     81\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while merging hparams: the keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minconsistent_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are present \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min both the LightningModule\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms and LightningDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms hparams \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut have different values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[0;32m     85\u001b[0m     hparams_initial \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlightning_hparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdatamodule_hparams}\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pl_module\u001b[38;5;241m.\u001b[39m_log_hyperparams:\n","\u001b[1;31mRuntimeError\u001b[0m: Error while merging hparams: the keys ['batch_size'] are present in both the LightningModule's and LightningDataModule's hparams but have different values."]}],"source":["predictions = trainer.predict(model, datamodule=dkasc_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'predictions' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_pred_vs_act(\u001b[43mpredictions\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m], predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m], ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPower ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"]}],"source":["plot_pred_vs_act(predictions[-2][0], predictions[-2][1], ylabel=\"Power ratio\")"]},{"cell_type":"markdown","metadata":{},"source":["# Building Target model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","            features=['temperature', 'humidity', 'GHI-Pyranometer', 'Power-EL'],\n","            label='Power-EL',\n","            input_size=36,\n","            horizon=1,\n","            batch_size=32,\n","            num_channels = [20] * 4,\n","            in_channels = 4,\n","            learning_rate=0.001,\n","            scale=False,\n","            train=True,\n","            train_dataset_size = 0.8,\n","            val_dataset_size=0.1,\n","            test_dataset_size=0.1,\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wandb_logger = WandbLogger(project=\"Transfer Learning DKASC dataset training\", log_model=\"all\", prefix=\"target_model\", reinit=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pv = pd.read_csv(\"/kaggle/input/pv-logs/Power-Clausthal(in).csv\", sep=';', decimal=',', usecols=['temperature', 'humidity', 'GHI-Pyranometer', 'Power-EL'])\n","# pv['Power-EL'] = pv['Power-EL'] / pv['Power-EL'].max() * 100\n","# pv.loc[pv['Power-EL'] <= 0.01, 'Power-EL'] = 0.02\n","# pv.loc[pv['GHI-Pyranometer'] <= 0.01, 'GHI-Pyranometer'] = 0.02\n","# pv.to_csv(\"TU-Clausthal-PV-2023-08-09.csv\", index=False)\n","# pv.describe()\n","# train_dataset_size = int(len(pv) * CONFIG['train_dataset_size'])\n","# val_dataset_size = train_dataset_size + int(len(pv) * CONFIG['val_dataset_size'])\n","\n","# if CONFIG[\"scale\"] != True:\n","#     train_dataset_csv = pv[:train_dataset_size].reset_index(drop=True)\n","# else:\n","#     ss = StandardScaler()\n","#     train_dataset_csv = pd.DataFrame(ss.fit_transform(pv[:train_dataset_size].reset_index(drop=True)))\n","    \n","# train_dataset_csv.to_csv(\"TU-Clausthal-PV-train.csv\", index=False)\n","# pv[train_dataset_size:val_dataset_size].reset_index(drop=True).to_csv(\"TU-Clausthal-PV-val.csv\", index=False)\n","# pv[val_dataset_size:].reset_index(drop=True).to_csv(\"TU-Clausthal-PV-test.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class PVTLDataModule(pl.LightningDataModule):\n","    def __init__(self, batch_size: int = 32):\n","\n","        super().__init__()\n","        self.save_hyperparameters(logger=False)\n","        self.batch_size = batch_size\n","    \n","    def prepare_data(self):\n","        pass\n","        \n","    def setup(self, stage=None):\n","        if stage == 'fit':\n","            self.pv_train = DKACS(\"/kaggle/working/TU-Clausthal-PV-train.csv\", CONFIG['horizon'], CONFIG['input_size'])\n","            self.pv_val = DKACS(\"/kaggle/working/TU-Clausthal-PV-val.csv\", CONFIG['horizon'], CONFIG['input_size'])\n","        \n","        if stage == 'test':\n","            self.pv_test = DKACS(\"/kaggle/working/TU-Clausthal-PV-test.csv\", CONFIG['horizon'], CONFIG['input_size'])\n","        \n","        if stage == 'predict':\n","            self.pv_test = DKACS(\"/kaggle/working/TU-Clausthal-PV-test.csv\", CONFIG['horizon'], CONFIG['input_size'])\n","\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.pv_train, shuffle=False, num_workers=3, batch_size=self.batch_size, drop_last=True)\n","    \n","    def val_dataloader(self):\n","        return DataLoader(self.pv_val, shuffle=False, num_workers=3, batch_size=self.batch_size, drop_last=True)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.pv_test, shuffle=False, num_workers=3, batch_size=self.batch_size, drop_last=True)\n","    \n","    def predict_dataloader(self):\n","        return DataLoader(self.pv_test, shuffle=False, num_workers=3, batch_size=self.batch_size, drop_last=True)\n","    \n","    def teardown(self, stage):\n","        if stage == 'fit':\n","            del self.pv_train\n","            del self.pv_val\n","            \n","        if stage == 'test':\n","            del self.pv_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TL(pl.LightningModule):\n","    def __init__(self, learning_rate=0.001):\n","        super().__init__()\n","        self.learning_rate = learning_rate\n","        model = TCNETANetLSTM.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\",\n","                                                   batch_size=CONFIG['batch_size'],\n","                                                   in_channels=4,\n","                                                   num_channels=CONFIG['num_channels'],\n","                                                  )\n","        model.freeze()\n","        self.feature_extractor = nn.Sequential(model.network, model.permute_layer, model.gru)\n","        self.flatten = nn.Flatten()\n","        self.criterion = nn.L1Loss()\n","        self.output_layer = nn.Linear(576, 1024)\n","        self.ol2 = nn.Linear(1024, 248)\n","        self.ol3 = nn.Linear(248, 1)\n","        self.net = nn.Sequential(self.output_layer, self.ol2, self.ol3)\n","        \n","    def _forward_features(self, x):\n","        x = self.feature_extractor(x)\n","        x = x[0]\n","        x = self.flatten(x)\n","        return x\n","    \n","    def _get_conv_output(self, shape):\n","        batch_size = 1\n","        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n","        output_feat = self._forward_features(tmp_input) \n","        n_size = output_feat.data.view(batch_size, -1).size(1)\n","        return n_size\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.net(x)\n","        return x\n","    \n","    def training_step(self, batch):\n","        batch, gt = batch[0], batch[1]\n","        out = self.forward(batch)\n","        loss = self.criterion(out, gt)\n","        self.log('loss_epoch', loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        batch, gt = batch[0], batch[1]\n","        out = self.forward(batch)\n","        loss = self.criterion(out, gt)\n","        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n","        return loss\n","    \n","    def test_step(self, *args, **kwargs):\n","        batch = kwargs.get(\"batch\", args[0])\n","        predictions = self(batch)\n","        actual_values = batch[1]\n","        loss = self.criterion(predictions, actual_values)\n","        loss_mse = nn.functional.mse_loss(predictions, actual_values)\n","        self.log('test_MAE_loss', loss, logger=True)\n","        self.log(\"test_mse_loss\", loss_mse, logger=True)\n","        return {\"prediction\": predictions, \"actual\": actual_values, \"loss_mae\": loss, \"loss_mse\": loss_mse}\n","    \n","    def predict_step(self, *args, **kwargs):\n","        batch = kwargs.get(\"batch\", args[0])\n","        predictions = self(batch)\n","        actual_values = batch[1]\n","        print(predictions, actual_values)\n","        return predictions, actual_values\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","\n","    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n","        plot_pred_vs_act(outputs['prediction'], outputs['actual'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'artifact_dir' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pv_datamodule \u001b[38;5;241m=\u001b[39m PVTLDataModule(batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m target_model \u001b[38;5;241m=\u001b[39m \u001b[43mTL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[44], line 5\u001b[0m, in \u001b[0;36mTL.__init__\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m TCNETANetLSTM\u001b[38;5;241m.\u001b[39mload_from_checkpoint(Path(\u001b[43martifact_dir\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                            batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m                                            in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                            num_channels\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_channels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m                                           )\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(model\u001b[38;5;241m.\u001b[39mnetwork, model\u001b[38;5;241m.\u001b[39mpermute_layer, model\u001b[38;5;241m.\u001b[39mgru)\n","\u001b[1;31mNameError\u001b[0m: name 'artifact_dir' is not defined"]}],"source":["pv_datamodule = PVTLDataModule(batch_size=CONFIG[\"batch_size\"])\n","target_model = TL(learning_rate=CONFIG['learning_rate'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["callbacks = [\n","        ModelCheckpoint(monitor=\"val_loss\", filename='{epoch}-{val_loss:.2f}'),\n","        EarlyStopping('val_loss', patience=5)\n","            ]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = pl.Trainer(max_epochs=1000, logger=wandb_logger, callbacks=callbacks, log_every_n_steps=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.test(model, pv_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["if CONFIG['train']:\n","    trainer.fit(target_model, datamodule=pv_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_id = \"model-y4piturf\"\n","# model_id = \"model-0xv5a270\"\n","\n","local_artifacts = list(Path(\"\").glob(f\"**/{model_id}*\"))\n","if not local_artifacts:\n","    artifact = wandb.run.use_artifact(f'hz33co/Transfer Learning DKASC dataset training/{model_id}:best', type='model')\n","    artifact_dir = artifact.download()\n","else:\n","    artifact_dir = local_artifacts[-1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_model = TL.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\", model=\"model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.test(model=target_model, datamodule=pv_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.test(model=model, datamodule=pv_datamodule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = trainer.predict(model=target_model, datamodule=pv_datamodule)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5273784,"sourceId":8774867,"sourceType":"datasetVersion"},{"datasetId":5050405,"sourceId":8792058,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
